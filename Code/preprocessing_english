import os
import pandas as pd
import numpy as np
import spacy
from sentence_transformers import SentenceTransformer
from tqdm import tqdm
from sklearn.model_selection import train_test_split
from sklearn.svm import LinearSVC
from sklearn.preprocessing import LabelEncoder
from sklearn.metrics import classification_report
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.metrics import confusion_matrix
from sklearn.preprocessing import StandardScaler


# Load multilingual spaCy models
nlp_models = {
    "en": spacy.load("en_core_web_sm"),
    "fr": spacy.load("fr_core_news_sm")
}

# Load multilingual document embedding model
doc_model = SentenceTransformer("paraphrase-multilingual-MiniLM-L12-v2")

def extract_features(text, lang="en"):
    nlp = nlp_models[lang]
    doc = nlp(text)

    num_tokens = len([t for t in doc if t.is_alpha])
    num_sentences = len(list(doc.sents)) or 1
    unique_tokens = len(set([t.lemma_.lower() for t in doc if t.is_alpha]))

    avg_sentence_len = num_tokens / num_sentences
    type_token_ratio = unique_tokens / num_tokens

    pos_counts = doc.count_by(spacy.attrs.POS)
    total_pos = sum(pos_counts.values())
    pos_ratios = {f"POS_{nlp.vocab[pos].text}": count / total_pos for pos, count in pos_counts.items()}

    punct_count = sum(1 for t in doc if t.is_punct)
    punct_ratio = punct_count / len(doc)

    features = {
        "avg_sentence_len": avg_sentence_len,
        "type_token_ratio": type_token_ratio,
        "punct_ratio": punct_ratio,
        **pos_ratios
    }

    return features

def process_documents_by_genre(base_path, lang="en"):
    all_features = []
    doc_vectors = []
    genres = []
    filenames = []

    # Loop over subfolders (genres)
    for genre in os.listdir(base_path):
        genre_path = os.path.join(base_path, genre)
        if not os.path.isdir(genre_path):
            continue

        for fname in tqdm(os.listdir(genre_path), desc=f"Processing {genre}"):
            if fname.endswith(".txt"):
                file_path = os.path.join(genre_path, fname)
                try:
                    with open(file_path, "r", encoding="utf-8") as f:
                        text = f.read()
                except UnicodeDecodeError:
                    with open(file_path, "r", encoding="latin-1") as f:
                        text = f.read()

                feats = extract_features(text, lang)
                vec = doc_model.encode(text)

                all_features.append(feats)
                doc_vectors.append(vec)
                genres.append(genre)
                filenames.append(fname)

    feature_df = pd.DataFrame(all_features).fillna(0)
    embeddings = np.vstack(doc_vectors)
    labels = pd.Series(genres, name="genre")

    return feature_df, embeddings, labels, filenames

base_path = "/home/clotilde/Documents/Cours/S2/Computational Linguistics/Project/Data/English"
feature_df, embeddings, labels, filenames = process_documents_by_genre(base_path, lang="en")

# Make sure results folder exists
os.makedirs("results_english", exist_ok=True)

# Save structural features to CSV
feature_df["genre"] = labels
feature_df["filename"] = filenames
feature_df.to_csv("results_english/structural_features.csv", index=False)

# Save embeddings to .npy (NumPy binary format)
np.save("results_english/doc_embeddings.npy", embeddings)

# Optional: Save labels separately
labels.to_csv("results_english/labels.csv", index=False)

# Load features and labels
features_df = pd.read_csv("results_english/structural_features.csv")
X = features_df.drop(columns=["genre", "filename"])
y = features_df["genre"]

# âœ… Save column names BEFORE normalizing
feature_names = X.columns

# ðŸ”„ Normalize the structural features
scaler = StandardScaler()
X = scaler.fit_transform(X)

# âœ… Stratified train/test split
X_train, X_test, y_train, y_test = train_test_split(
    X, y, test_size=0.2, random_state=42, stratify=y)

# Encode class labels for SVM
le = LabelEncoder()
y_train_enc = le.fit_transform(y_train)
y_test_enc = le.transform(y_test)

# ðŸ”§ Train the Linear SVM
svm_struct = LinearSVC(C=1, max_iter=10000)
svm_struct.fit(X_train, y_train_enc)

# ðŸŽ¯ Evaluate
y_pred = svm_struct.predict(X_test)
print(classification_report(y_test_enc, y_pred, target_names=le.classes_))

# ðŸ“Š Top 10 Features Per Genre (X is already normalized!)
coefs = svm_struct.coef_
class_labels = le.classes_

# Save top features per genre to file
with open("results_english/top_features_per_genre.txt", "w", encoding="utf-8") as f:
    for i, class_name in enumerate(class_labels):
        class_coefs = coefs[i]
        top_idx = np.argsort(np.abs(class_coefs))[::-1][:10]
        f.write(f"ðŸŽ­ Genre: {class_name}\n")
        for idx in top_idx:
            fname = feature_names[idx]  # This works because we saved it before normalization
            weight = class_coefs[idx]
            f.write(f"   {fname:<25} â†’ weight: {weight:+.4f}\n")
        f.write("\n")


print("\nðŸ” Top 10 Features per Genre:\n")
for i, class_name in enumerate(class_labels):
    class_coefs = coefs[i]
    top_idx = np.argsort(np.abs(class_coefs))[::-1][:10]
    print(f"ðŸŽ­ Genre: {class_name}")
    for idx in top_idx:
        fname = feature_names[idx]
        weight = class_coefs[idx]
        print(f"   {fname:<25} â†’ weight: {weight:+.4f}")
    print()

# ðŸ§© Confusion Matrix
cm = confusion_matrix(y_test_enc, y_pred)
plt.figure(figsize=(10, 8))
sns.heatmap(cm, annot=True, fmt="d", xticklabels=le.classes_, yticklabels=le.classes_, cmap="Blues")
plt.title("Confusion Matrix")
plt.xlabel("Predicted")
plt.ylabel("True")
plt.tight_layout()

# Save to results_english folder
plt.savefig("results_english/confusion_matrix.png")
plt.close()
print("âœ… Confusion matrix saved to results_english/confusion_matrix.png")
